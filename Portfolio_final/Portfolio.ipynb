{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using Tensorflow \n",
    "\n",
    "### Authors : Renu Hadke, Menita Koonani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective \n",
    "Our objective is to identify and classify objects spotted on images and real-time video using TensorFlow and to determine the accuracy of each identification. We are considering two Models namely, SSD with MobileNet, SSD Inception V2 model and Faster RCNN Inception model to compare the accuracy and size of the models.\n",
    "The principal difference between the models is that Faster RCNN Inception V2 is optimized for accuracy, while the MobileNets are optimized to be small and efficient, at the cost of some accuracy. The SSD with MobileNets detects objects in only a single shot with just two components in its architecture namely, Feature Extraction and Detection Generator, while the Faster R-CNN consists of three components- Feature Extraction, Proposal Generation and Box Classifier.\n",
    "\n",
    "In the Faster R-CNN Inception model, a region proposal network is used to generate regions of interest and then either fully-connected layers or position-sensitive convolutional layers to classify those regions. SSD does the two in a “single shot,” simultaneously predicting the bounding box and the class as it processes the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the terms\n",
    "Let us first understand few terms before we jump to the process of object detection and comparing the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (CNN):\n",
    "\n",
    "It is a class of deep, feed-forward artificial neural networks that has been applied to analyzing visual imagery. \n",
    "These are a special class of Multilayer perceptron which are well suited for pattern classification. \n",
    "It is specifically designed to recognize 2D shapes with a high level of invariance, skewing and scaling. \n",
    "They are made up of neurons that have learnable weights and biases. Each neuron receives some input, performs a dot product and optionally follows it with a non-linearity. \n",
    "The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. A simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function.\n",
    "There are three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer\n",
    "![title](images/cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD with MobileNet:\n",
    "Out of the many detection models, we chose to work with the combination of Single Shot Detectors(SSDs) and MobileNets architecture as they are fast, efficient and do not require huge computational capability to fulfill the Object Detection task. The SSD approach is based on a feed-forward convolutional neural network which produces a fixed-size collection of bounding boxes and scores for the object class instances present in those boxes.\n",
    "\n",
    "The main difference between a “traditional” CNN’s and the MobileNet architecture is instead of a single 3x3 convolution layer followed by batch norm and ReLU, MobileNets split the convolution into a 3x3 depthwise conv and a 1x1 pointwise conv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSD Inception V2 Model:\n",
    "\n",
    "Given an input image and a set of ground truth labels, SSD does the following:\n",
    "\n",
    "•\tIt passes the image through a series of convolutional layers, providing several sets of feature maps at different scales.\n",
    "•\tFor each location in each of these feature maps, a 3x3 convolutional filter is used to evaluate a small set of default bounding boxes.\n",
    "•\tFor each box, it simultaneously predicts the bounding box offset and the probabilities of each class.\n",
    "•\tDuring training, it matches the ground truth box with these predicted boxes based on IoU(Intersection over Union). The best predicted box is labeled a “positive” along with all the other boxes having an IoU with the truth greater than 0.5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster RCNN Inception Model:\n",
    "\n",
    "The main insight of Faster R-CNN was to replace the slow selective search algorithm that was used in the R-CNN (Region-based Convolutional Neural Network), with a fast neural net. \n",
    "Faster R-CNN is similar to the original R-CNN but is improved on its detection speed  through two augmentations:\n",
    "\n",
    "•\tIt performs feature extraction over the image even before proposing regions, thus running only one CNN over the entire image instead of running 2000 CNN’s across 2000 overlapping regions\n",
    "•\tIt replaces the SVM with a softmax layer, thus extending the network for predictions instead of creating a new model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow:\n",
    "\n",
    "TensorFlow is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV:\n",
    "\n",
    "OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at real-time computer vision. The C++ API provides a class ‘videocapture’ for capturing video from cameras or for reading video files and image sequences. It is basically used to access the Webcam of our computer to capture real-time videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained dataset: \n",
    "COCO Dataset is a large-scale object detection, segmentation and captioning dataset. It is downloaded from https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md and it has 200K labelled images categorized into 90 classes.\n",
    "\n",
    "Choose any dataset you want from the following list\n",
    "![title](images/models.png)\n",
    "\n",
    "### COCO mAP:\n",
    "The higher the mAp (minimum average precision), the better the model. Based on the observations, SSD with MobileNets provided much better results in terms of speed but the Faster RCNN Inception Model provided a higher accuracy with some compromise on the speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code With Documentation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install tensorflow in anaconda using the below command : \n",
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenCV using following command:\n",
    "conda install -c https://conda.binstar.org/menpo opencv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from utils import label_map_util\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "# in order to display the images in line\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model URL\n",
    "The model would download from the below url\n",
    "\n",
    "download_url = 'http://download.tensorflow.org/models/object_detection/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To change the model name according of your choice, change the following code\n",
    "\"model = 'ssd_mobilenet_v1_coco_11_06_2017'\" to\n",
    "\"model = 'faster_rcnn_inception_v2_coco_2018_01_28\"\n",
    "![title](images/ssd_model.png)\n",
    "![title](images/fasterRCNN_model.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protobuf file:\n",
    "Protocol Buffers is a method of serializing structured data. The pb file is a json file for us. You will find all the genral classes that are considered for detection.\n",
    "You will find a protobuf file in the <folder> in the given github link\n",
    "<githublink>\n",
    "    A snapshot of the protobuf file (mscoco_label_map.pbtxt)\n",
    "![title](images/protobuf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the tar and downloading the model you have chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opens the tar file and downloads the model to our system\n",
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(download_url + model_tar, model_tar)\n",
    "file = tarfile.open(model_tar)\n",
    "print(file)\n",
    "for each_file in file.getmembers():\n",
    "    each_file_name = os.path.basename(each_file.name)\n",
    "    if 'frozen_inference_graph.pb' in each_file_name:\n",
    "        file.extract(each_file, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading frozen TF model in the memory by creating a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a frozen tensorflow model into memory\n",
    "graph_detection = tf.Graph()\n",
    "with graph_detection.as_default():\n",
    "    graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(path, 'rb') as fid:\n",
    "        graph_serialized = fid.read()\n",
    "        graph_def.ParseFromString(graph_serialized)\n",
    "        tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the lables from the frozen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading labels and their mappings\n",
    "label_map = label_map_util.load_labelmap(label_path)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=classes_num, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of an image:\n",
    "We are loading the dimensions of each image into a numpy array. The dimensions of the image are the height, the width and the RGB intensity at various points in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images into a numpy array which consists of the dimensions of each image\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving path for images to detect objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the test images are stored\n",
    "test_images_dir = 'test_images'\n",
    "test_image_path = [ os.path.join(test_images_dir, 'ILSVRC2017_test_00000013.jpeg')]\n",
    "\n",
    "# Size of the output images in inches\n",
    "image_size = (8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting objects in an image or set of images\n",
    "The below code will be used for detecting objects in an image or a set of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph_detection.as_default():\n",
    "    with tf.Session(graph=graph_detection) as sess:\n",
    "        for image_path in test_image_path:\n",
    "            \n",
    "            # opening images from the path\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            # array representation of the image used later to prepare the result image with bounding boxes with labels on it\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            \n",
    "            # expanding the dimensions of the image as the model expects images to have the shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            image_tensor = graph_detection.get_tensor_by_name('image_tensor:0')\n",
    "            \n",
    "            # each box represents parts of the image where a particular object was detected\n",
    "            boxes = graph_detection.get_tensor_by_name('detection_boxes:0')\n",
    "            \n",
    "            # each score represents the level of confidence for each of the objects\n",
    "            # this score is shown on the result image along with the class label\n",
    "            scores = graph_detection.get_tensor_by_name('detection_scores:0')\n",
    "            classes = graph_detection.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = graph_detection.get_tensor_by_name('num_detections:0')\n",
    "            \n",
    "            # Actual detection\n",
    "            (boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections],\n",
    "              feed_dict={image_tensor: image_np_expanded})\n",
    "            \n",
    "            # Visualization of the results of an indentified object\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=8)\n",
    "            plt.figure(figsize=image_size)\n",
    "            plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting objects through webcam\n",
    "The below code will be used to detect objects through webcam.\n",
    "To capture a video, we need to create a VideoCapture object. Its argument can be either the device index or the name of a video file. Device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap=cv2.VideoCapture(0)\n",
    "ret = True\n",
    "with graph_detection.as_default():\n",
    "    with tf.Session(graph=graph_detection) as sess:\n",
    "   \n",
    "     while(ret):\n",
    "        ret,image_np=cap.read()\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "        image_tensor = graph_detection.get_tensor_by_name('image_tensor:0')\n",
    "        \n",
    "      # Each box represents a part of the image where a particular object was detected.\n",
    "        boxes = graph_detection.get_tensor_by_name('detection_boxes:0')\n",
    "      # Each score represent how level of confidence for each of the objects.\n",
    "      # Score is shown on the result image, together with the class label.\n",
    "        scores = graph_detection.get_tensor_by_name('detection_scores:0')\n",
    "        classes = graph_detection.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = graph_detection.get_tensor_by_name('num_detections:0')\n",
    "      # Actual detection.\n",
    "        (boxes, scores, classes, num_detections) = sess.run(\n",
    "          [boxes, scores, classes, num_detections],\n",
    "          feed_dict={image_tensor: image_np_expanded})\n",
    "      # Visualization of the results of a detection.\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32),\n",
    "        np.squeeze(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "        cv2.imshow('image',cv2.resize(image_np,(1280,960)))\n",
    "        if cv2.waitKey(25) & 0xFF==ord('q'):\n",
    "            break\n",
    "            cv2.destroyAllWindows()\n",
    "            cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the results that we got.\n",
    "\n",
    "For **SSD with MobileNet**, the accuracy of object detection in image is 83% for person and 81% for laptop. This model worked fast though had the least accuracy.\n",
    "\n",
    "![title](images/ssd_results.png)\n",
    "\n",
    "For **SSD Inception V2 model**, the accuracy of the objects detected in the images is 90% for the person and 95% for the laptop.\n",
    "\n",
    "![title](images/ssdV2_result.png)\n",
    "\n",
    "For **Faster RCNN Inception Model**, the accuracy of object detection in image is 99% for person and 99% for laptop\n",
    "\n",
    "![title](images/fasterRCNN_result.png)\n",
    "\n",
    "In addition, we were also successful in accessing the webcam of our system using OpenCV to detect real-time objects. **The model used here is SSD with MobileNets as it produces much faster results as compared to the other two models.**\n",
    "![title](images/real_time_detect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SSD with MobileNet model accuracy:**\n",
    "Person 83% and laptop 81%\n",
    "\n",
    "**SSD Inception V2 model accuracy:**\n",
    "Person 90% and laptop 95%\n",
    "\n",
    "**Faster RCNN Inception Model accuracy:**\n",
    "Person 99% and laptop 99%\n",
    "\n",
    "As we can see above, Faster RCNN Inception Model gives the highest accuracy and SSD with MobileNet gives the lowest.\n",
    "But MobileNets are optimized to be small and efficient, at the cost of some accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "    \n",
    "    https://github.com/tensorflow/models/tree/master/research/object_detection\n",
    "    https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### License:\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2018 renuHadke, menitakoonani\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the document by Menita Koonani and Renu Hadke is licensed under the MIT License https://opensource.org/licenses/MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
